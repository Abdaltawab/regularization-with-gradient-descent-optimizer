{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395fa1a0",
   "metadata": {},
   "source": [
    "# Step 1: Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f710a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedf8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows * cols)\n",
    "        images = images.astype(np.float64) / 255.0  # normalize to [0, 1]\n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1447e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e047d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_mnist_images('train-images.idx3-ubyte')\n",
    "y = load_mnist_labels('train-labels.idx1-ubyte')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd239561",
   "metadata": {},
   "source": [
    "# Subset the dataset to use only class 0 and class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726dea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((y == 0) | (y == 1))[0]\n",
    "X = X[idx]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4cf87",
   "metadata": {},
   "source": [
    "# Standardize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdcbec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_std[X_std == 0] = 0.00000000000000001  # avoid division by zero\n",
    "X = (X - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290da40",
   "metadata": {},
   "source": [
    "# Add a column of ones to X for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59505057",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((len(X), 1)), X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a2031",
   "metadata": {},
   "source": [
    "# Step 3: Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699eba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the loss function\n",
    "def compute_loss(X, y, weights, lambda_reg):\n",
    "    m = X.shape[0]\n",
    "    h = sigmoid(np.dot(X, weights))\n",
    "    reg_term = (lambda_reg / (2 * m)) * np.sum(np.square(weights[1:]))\n",
    "    loss = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + reg_term\n",
    "    return loss\n",
    "\n",
    "# Define the gradient of the loss function\n",
    "def compute_grad(X, y, weights, lambda_reg):\n",
    "    m = X.shape[0]\n",
    "    h = sigmoid(np.dot(X, weights))\n",
    "    grad = (1 / m) * np.dot(X.T, (h - y))\n",
    "    grad[1:] += (lambda_reg / m) * weights[1:]\n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65112a",
   "metadata": {},
   "source": [
    "# Implement Logistic Regression with L1 regularization and gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e1076d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Logistic Regression with L1 regularization and gradient descent optimizer\n",
    "def logistic_regression_l1(X, y, lambda_reg, learning_rate, num_iterations):\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        loss = compute_loss(X, y, weights, lambda_reg)\n",
    "        grad = compute_grad(X, y, weights, lambda_reg)\n",
    "        \n",
    "        weights -= learning_rate * grad\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return weights, losses\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_1 = 0.1\n",
    "lambda_2 = 0.01\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1000\n",
    "\n",
    "# Train logistic regression model with L1 regularization (lambda = 0.1)\n",
    "weights_l1_1, losses_l1_1 = logistic_regression_l1(X, y, lambda_1, learning_rate, num_iterations)\n",
    "\n",
    "# Train logistic regression model with L1 regularization (lambda = 0.01)\n",
    "weights_l1_2, losses_l1_2 = logistic_regression_l1(X, y, lambda_2, learning_rate, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d3413",
   "metadata": {},
   "source": [
    "# Step 7: Use mini-batch gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e631fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement mini-batch gradient descent optimizer\n",
    "def mini_batch_gradient_descent(X, y, lambda_reg, learning_rate, num_epochs, batch_size):\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    losses = []\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    num_batches = int(np.ceil(m / batch_size))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = min((batch + 1) * batch_size, m)\n",
    "            \n",
    "            X_batch = X[start_idx:end_idx]\n",
    "            y_batch = y[start_idx:end_idx]\n",
    "            \n",
    "            loss = compute_loss(X_batch, y_batch, weights, lambda_reg)\n",
    "            grad = compute_grad(X_batch, y_batch, weights, lambda_reg)\n",
    "            \n",
    "            weights -= learning_rate * grad\n",
    "            losses.append(loss)\n",
    "    \n",
    "    return weights, losses\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "batch_size_1 = 64\n",
    "batch_size_2 = 128\n",
    "\n",
    "# Train logistic regression model using mini-batch gradient descent (batch size = 64)\n",
    "weights_mb_1, losses_mb_1 = mini_batch_gradient_descent(X, y, lambda_1, learning_rate, num_epochs, batch_size_1)\n",
    "\n",
    "# Train logistic regression model using mini-batch gradient descent (batch size = 128)\n",
    "weights_mb_2, losses_mb_2 = mini_batch_gradient_descent(X, y, lambda_1, learning_rate, num_epochs, batch_size_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6cce0e",
   "metadata": {},
   "source": [
    "# Step 8: Use RMSProp optimizer and Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "972d57a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\workstation\\AppData\\Local\\Temp\\ipykernel_7016\\3266782624.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + reg_term\n",
      "C:\\Users\\workstation\\AppData\\Local\\Temp\\ipykernel_7016\\3266782624.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + reg_term\n"
     ]
    }
   ],
   "source": [
    "# Implement RMSProp optimizer\n",
    "def rmsprop(X, y, lambda_reg, learning_rate, num_iterations, epsilon, decay_rate):\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    cache = np.zeros(X.shape[1])\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        loss = compute_loss(X, y, weights, lambda_reg)\n",
    "        grad = compute_grad(X, y, weights, lambda_reg)\n",
    "        \n",
    "        cache = decay_rate * cache + (1 - decay_rate) * np.square(grad)\n",
    "        weights -= learning_rate * grad / (np.sqrt(cache) + epsilon)\n",
    "        \n",
    "        losses.append(loss)\n",
    "    \n",
    "    return weights, losses\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon = 1e-8\n",
    "decay_rate = 0.9\n",
    "num_iterations = 1000\n",
    "\n",
    "# Train logistic regression model using RMSProp optimizer\n",
    "weights_rmsprop, losses_rmsprop = rmsprop(X, y, lambda_1, learning_rate, num_iterations, epsilon, decay_rate)\n",
    "\n",
    "\n",
    "# Implement Adam optimizer\n",
    "def adam(X, y, lambda_reg, learning_rate, num_iterations, epsilon, beta1, beta2):\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    m = np.zeros(X.shape[1])\n",
    "    v = np.zeros(X.shape[1])\n",
    "    t = 0\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        t += 1\n",
    "        loss = compute_loss(X, y, weights, lambda_reg)\n",
    "        grad = compute_grad(X, y, weights, lambda_reg)\n",
    "        \n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * np.square(grad)\n",
    "        \n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        weights -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        losses.append(loss)\n",
    "    \n",
    "    return weights, losses\n",
    "\n",
    "# Hyperparameters\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "num_iterations = 1000\n",
    "\n",
    "# Train logistic regression model using Adam optimizer\n",
    "weights_adam, losses_adam = adam(X, y, lambda_1, learning_rate, num_iterations, epsilon, beta1, beta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f4853",
   "metadata": {},
   "source": [
    "# Step 9: Report the accuracies and write conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e9a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def predict(X, weights):\n",
    "    return sigmoid(np.dot(X, weights))\n",
    "\n",
    "# Evaluate accuracy on the validation set\n",
    "def accuracy(X, y, weights):\n",
    "    y_pred = np.round(predict(X, weights))\n",
    "    return np.mean(y_pred == y)\n",
    "\n",
    "# Calculate accuracies for each case\n",
    "\n",
    "# Logistic Regression with L1 regularization (lambda = 0.1)\n",
    "accuracy_l1_1 = accuracy(X, y, weights_l1_1)\n",
    "\n",
    "# Logistic Regression with L1 regularization (lambda = 0.01)\n",
    "accuracy_l1_2 = accuracy(X, y, weights_l1_2)\n",
    "\n",
    "# Mini-batch Gradient Descent (batch size = 64)\n",
    "accuracy_mb_1 = accuracy(X, y, weights_mb_1)\n",
    "\n",
    "# Mini-batch Gradient Descent (batch size = 128)\n",
    "accuracy_mb_2 = accuracy(X, y, weights_mb_2)\n",
    "\n",
    "# RMSProp optimizer\n",
    "accuracy_rmsprop = accuracy(X, y, weights_rmsprop)\n",
    "\n",
    "# Adam optimizer\n",
    "accuracy_adam = accuracy(X,y, weights_adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b52352a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression with L1 regularization (lambda = 0.1): 99.94%\n",
      "Accuracy for Logistic Regression with L1 regularization (lambda = 0.01): 99.94%\n",
      "Accuracy for Mini-batch Gradient Descent (batch size = 64): 99.99%\n",
      "Accuracy for Mini-batch Gradient Descent (batch size = 128): 99.99%\n",
      "Accuracy for RMSProp optimizer: 100.00%\n",
      "Accuracy for Adam optimizer: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracies for each case\n",
    "\n",
    "# Logistic Regression with L1 regularization (lambda = 0.1)\n",
    "accuracy_l1_1 = accuracy(X, y, weights_l1_1)\n",
    "\n",
    "# Logistic Regression with L1 regularization (lambda = 0.01)\n",
    "accuracy_l1_2 = accuracy(X, y, weights_l1_2)\n",
    "\n",
    "# Mini-batch Gradient Descent (batch size = 64)\n",
    "accuracy_mb_1 = accuracy(X, y, weights_mb_1)\n",
    "\n",
    "# Mini-batch Gradient Descent (batch size = 128)\n",
    "accuracy_mb_2 = accuracy(X, y, weights_mb_2)\n",
    "\n",
    "# RMSProp optimizer\n",
    "accuracy_rmsprop = accuracy(X, y, weights_rmsprop)\n",
    "\n",
    "# Adam optimizer\n",
    "accuracy_adam = accuracy(X, y, weights_adam)\n",
    "\n",
    "# Report the accuracies and write conclusions\n",
    "\n",
    "print(\"Accuracy for Logistic Regression with L1 regularization (lambda = 0.1): {:.2f}%\".format(accuracy_l1_1 * 100))\n",
    "print(\"Accuracy for Logistic Regression with L1 regularization (lambda = 0.01): {:.2f}%\".format(accuracy_l1_2 * 100))\n",
    "print(\"Accuracy for Mini-batch Gradient Descent (batch size = 64): {:.2f}%\".format(accuracy_mb_1 * 100))\n",
    "print(\"Accuracy for Mini-batch Gradient Descent (batch size = 128): {:.2f}%\".format(accuracy_mb_2 * 100))\n",
    "print(\"Accuracy for RMSProp optimizer: {:.2f}%\".format(accuracy_rmsprop * 100))\n",
    "print(\"Accuracy for Adam optimizer: {:.2f}%\".format(accuracy_adam * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3443e46b",
   "metadata": {},
   "source": [
    "# Write conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f56ed158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conclusions ---\n",
      "The logistic regression model with L1 regularization (lambda = 0.1) achieved an accuracy of 99.94%.\n",
      "The logistic regression model with L1 regularization (lambda = 0.01) achieved an accuracy of 99.94%.\n",
      "The mini-batch gradient descent optimizer with batch size 64 achieved an accuracy of 99.99%.\n",
      "The mini-batch gradient descent optimizer with batch size 128 achieved an accuracy of 99.99%.\n",
      "The RMSProp optimizer achieved an accuracy of 100.00%.\n",
      "The Adam optimizer achieved an accuracy of 100.00%.\n",
      "\n",
      "The logistic regression model with L1 regularization and a larger lambda value (lambda = 0.1) may have resulted in a lower accuracy compared to the model with a smaller lambda value (lambda = 0.01). This is because a larger lambda value imposes a stronger regularization penalty, which can lead to underfitting if the regularization is too strong.\n",
      "The mini-batch gradient descent optimizer with a batch size of 64 achieved a similar accuracy to the one with a batch size of 128. The choice of batch size can impact the convergence speed and generalization of the model, but in this case, the difference in accuracy between the two batch sizes was not significant.\n",
      "Both the RMSProp optimizer and the Adam optimizer achieved high accuracies. These optimizers adapt the learning rate based on the gradients, which can improve convergence and performance compared to standard gradient descent methods.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n--- Conclusions ---\")\n",
    "\n",
    "print(\"The logistic regression model with L1 regularization (lambda = 0.1) achieved an accuracy of {:.2f}%.\".format(accuracy_l1_1 * 100))\n",
    "print(\"The logistic regression model with L1 regularization (lambda = 0.01) achieved an accuracy of {:.2f}%.\".format(accuracy_l1_2 * 100))\n",
    "print(\"The mini-batch gradient descent optimizer with batch size 64 achieved an accuracy of {:.2f}%.\".format(accuracy_mb_1 * 100))\n",
    "print(\"The mini-batch gradient descent optimizer with batch size 128 achieved an accuracy of {:.2f}%.\".format(accuracy_mb_2 * 100))\n",
    "print(\"The RMSProp optimizer achieved an accuracy of {:.2f}%.\".format(accuracy_rmsprop * 100))\n",
    "print(\"The Adam optimizer achieved an accuracy of {:.2f}%.\".format(accuracy_adam * 100))\n",
    "\n",
    "print(\"\\nThe logistic regression model with L1 regularization and a larger lambda value (lambda = 0.1) may have resulted in a lower accuracy compared to the model with a smaller lambda value (lambda = 0.01). This is because a larger lambda value imposes a stronger regularization penalty, which can lead to underfitting if the regularization is too strong.\")\n",
    "\n",
    "print(\"The mini-batch gradient descent optimizer with a batch size of 64 achieved a similar accuracy to the one with a batch size of 128. The choice of batch size can impact the convergence speed and generalization of the model, but in this case, the difference in accuracy between the two batch sizes was not significant.\")\n",
    "\n",
    "print(\"Both the RMSProp optimizer and the Adam optimizer achieved high accuracies. These optimizers adapt the learning rate based on the gradients, which can improve convergence and performance compared to standard gradient descent methods.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb3db8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
